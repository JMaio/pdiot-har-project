{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('.')/'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'s1621503': 84,\n",
       " 's1704037': 80,\n",
       " 's1701688': 84,\n",
       " 's1737472': 0,\n",
       " 's1758009': 78,\n",
       " 's1710228': 78,\n",
       " 's1660711': 36,\n",
       " 's1655560': 78,\n",
       " 's2017768': 8,\n",
       " 's1721039': 76,\n",
       " 's1746788': 84,\n",
       " 's1711507': 97,\n",
       " 's1642301': 84,\n",
       " 's1764751': 20,\n",
       " 's1616573': 94}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_folder = data_root/'2020'\n",
    "student_pattern = \"s\\d{7}\"\n",
    "\n",
    "files = {}\n",
    "for (dirpath, dirnames, filenames) in os.walk(target_folder):\n",
    "  s_match = re.search(student_pattern, dirpath)\n",
    "  # if regex matches, and files exist\n",
    "  if s_match and filenames:\n",
    "    s = s_match.group()\n",
    "    # print(f\"adding files for {s} (in {dirpath})\")\n",
    "    dir = Path(dirpath)\n",
    "    try: files[s]\n",
    "    except KeyError: files[s] = []\n",
    "    # only accept .csv files\n",
    "    files[s].extend([dir/f for f in filenames if f[-4:] == '.csv'])\n",
    "\n",
    "print(\"File count:\")\n",
    "{k: len(v) for (k, v) in files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_CODE_TO_TFCODE_MAPPING = {\n",
    "  0:   0,   # \"Sitting\",\n",
    "  4:   1,   # \"Sitting bent forward\",\n",
    "  5:   2,   # \"Sitting bent backward\",\n",
    "  1:   3,   # \"Walking at normal speed\",\n",
    "  100: 4,   # \"Standing\",\n",
    "  2:   5,   # \"Lying down on back\",\n",
    "  7:   6,   # \"Lying down left\",\n",
    "  6:   7,   # \"Lying down right\",\n",
    "  8:   8,   # \"Lying down on stomach\",\n",
    "  9:   9,   # \"Movement\",\n",
    "  11:  10,  # \"Running\",\n",
    "  12:  11,  # \"Climbing stairs\",\n",
    "  13:  12,  # \"Descending stairs\",\n",
    "  31:  13,  # \"Desk work\"\n",
    "}\n",
    "\n",
    "ACTIVITY_TFCODE_TO_CODE_MAPPING = {\n",
    "  0:  0,   # \"Sitting\",\n",
    "  1:  4,   # \"Sitting bent forward\",\n",
    "  2:  5,   # \"Sitting bent backward\",\n",
    "  3:  1,   # \"Walking at normal speed\",\n",
    "  4:  100, # \"Standing\",\n",
    "  5:  2,   # \"Lying down on back\",\n",
    "  6:  7,   # \"Lying down left\",\n",
    "  7:  6,   # \"Lying down right\",\n",
    "  8:  8,   # \"Lying down on stomach\",\n",
    "  9:  9,   # \"Movement\",\n",
    "  10: 11,  # \"Running\",\n",
    "  11: 12,  # \"Climbing stairs\",\n",
    "  12: 13,  # \"Descending stairs\",\n",
    "  13: 31,  # \"Desk work\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "header_size = 5\n",
    "\n",
    "@dataclass\n",
    "class Header:\n",
    "  sensor_pos: str\n",
    "  sensor_side: str\n",
    "  act_type: str\n",
    "  act_code: np.ndarray\n",
    "  subject_id: str\n",
    "  # https://stackoverflow.com/a/54863771/9184658\n",
    "  def __post_init__(self):\n",
    "    self.act_code = ACTIVITY_CODE_TO_TFCODE_MAPPING[int(self.act_code)]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Activity:\n",
    "  header: Header\n",
    "  df: pd.DataFrame\n",
    "\n",
    "def read_custom_file(filename):\n",
    "  with open(filename) as f:\n",
    "    head = [next(f).rstrip().split('# ')[1] for x in range(header_size)]\n",
    "    header = Header(*[s.split(': ')[1] for s in head])\n",
    "    # for l in head:\n",
    "    #   print(l)\n",
    "    # print(header)\n",
    "\n",
    "    df = pd.read_csv(filename, header=header_size)\n",
    "    return Activity(header, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_respeck_fig(df):\n",
    "  fig = plt.figure(figsize=(10, 8))\n",
    "  plt.plot(df['accel_x'], label=\"accel_x\")\n",
    "  plt.plot(df['accel_y'], label=\"accel_y\")\n",
    "  plt.plot(df['accel_z'], label=\"accel_z\")\n",
    "  plt.legend()\n",
    "  return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Activity(header=Header(sensor_pos='Wrist', sensor_side='Right', act_type='Lying down right', act_code=7, subject_id='s1621503'), df=         timestamp  seq   accel_x   accel_y   accel_z\n",
       "0    1601826499860    0  0.080566 -0.230530 -0.979065\n",
       "1    1601826499938    1  0.081055 -0.230042 -0.978577\n",
       "2    1601826500017    2  0.084961 -0.233215 -0.978333\n",
       "3    1601826500096    3  0.083740 -0.231750 -0.979797\n",
       "4    1601826500175    4  0.084473 -0.232483 -0.981750\n",
       "..             ...  ...       ...       ...       ...\n",
       "411  1601826532226  411  0.060547 -0.211487 -0.983704\n",
       "412  1601826532305  412  0.060547 -0.207825 -0.989075\n",
       "413  1601826532383  413  0.060303 -0.204407 -0.987366\n",
       "414  1601826532462  414  0.061279 -0.208313 -0.986145\n",
       "415  1601826532541  415  0.061523 -0.207092 -0.984924\n",
       "\n",
       "[416 rows x 5 columns])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = read_custom_file(files[\"s1621503\"][0])\n",
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"accel_x\", \"accel_y\", \"accel_z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_pos = 'Chest'\n",
    "sensor_side = 'Right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split students into train and test set\n",
    "\n",
    "students = list(files.keys())\n",
    "    \n",
    "split = int(len(students) * 0.8)\n",
    "train_students = students[:split]\n",
    "test_students = students[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activities(files, students, window_size, sensor_pos, sensor_side):\n",
    "    student_files = [files[student] for student in students]\n",
    "    \n",
    "    activities = [read_custom_file(filename) for student in student_files for filename in student]\n",
    "    activities = [activity for activity in activities if not activity.df.empty and len(activity.df.index) >= window_size and activity.header.sensor_pos == sensor_pos and activity.header.sensor_side == sensor_side]\n",
    "    \n",
    "    return activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_activities = get_activities(files, train_students, window_size, sensor_pos, sensor_side)\n",
    "\n",
    "test_activities = get_activities(files, test_students, window_size, sensor_pos, sensor_side)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data\n",
    "# Y = labels\n",
    "\n",
    "def create_data_arrays(activities):\n",
    "    data = np.array([activity.df.get(keys).values for activity in activities], dtype=object)\n",
    "    labels = np.array([activity.header.act_code for activity in activities])\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw, train_labels = create_data_arrays(train_activities)\n",
    "\n",
    "test_data_raw, test_labels = create_data_arrays(test_activities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of output labels\n",
    "\n",
    "train_labels = np.array(pd.get_dummies(train_labels))\n",
    "test_labels = np.array(pd.get_dummies(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    x = (X - np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "    return x\n",
    "\n",
    "def standardize_array(data_raw):\n",
    "    data = np.zeros(data_raw.shape, dtype=object)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = (standardize(data_raw[i]))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# standardize input data\n",
    "\n",
    "train_data = standardize_array(train_data_raw)\n",
    "\n",
    "test_data = standardize_array(test_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(data_array, label_array, window_size, step_size):\n",
    "    windowed_data = []\n",
    "    windowed_labels = []\n",
    "    \n",
    "    for i in range(len(label_array)):\n",
    "        arr = data_array[i]\n",
    "        label = label_array[i]\n",
    "        start_i = 0\n",
    "        end_i = start_i + window_size\n",
    "\n",
    "        while end_i < arr.shape[0]:\n",
    "            windowed_data.append(arr[start_i:end_i])\n",
    "            windowed_labels.append(label)\n",
    "            start_i = start_i + step_size\n",
    "            end_i = start_i + window_size\n",
    "        \n",
    "    return np.array(windowed_data), np.array(windowed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10384, 100, 3)\n",
      "(10384, 14)\n",
      "(2684, 100, 3)\n",
      "(2684, 14)\n"
     ]
    }
   ],
   "source": [
    "train_data_win, train_label_win = window_data(train_data, train_labels, window_size, step_size)\n",
    "\n",
    "test_data_win, test_label_win = window_data(test_data, test_labels, window_size, step_size)\n",
    "\n",
    "print(train_data_win.shape)\n",
    "print(train_label_win.shape)\n",
    "\n",
    "print(test_data_win.shape)\n",
    "print(test_label_win.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# variable length inputs\n",
    "# use batch size = 1\n",
    "\n",
    "x = train_data_win\n",
    "y = train_label_win\n",
    "\n",
    "learn_rate = 0.0001\n",
    "batch_size = 600\n",
    "num_epochs = 1500\n",
    "\n",
    "num_channels = x.shape[2]\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "seq_len = x.shape[1]\n",
    "\n",
    "num_inputs = x.shape[0]\n",
    "num_outputs = y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy, r):\n",
    "    verbose, epochs, batch_size = 0, 10, 32\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    save_model(model, r)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "def run_experiment(trainX, trainy, testX, testy, repeats=1):\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    models = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy, r)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "        models.append\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    \n",
    "def save_model(model, n):\n",
    "    # Convert the model.\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    model_filename = 'cnn_model_' + str(n) + '.tflite' \n",
    "    # Save the model.\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 34.948\n",
      "[34.94783937931061]\n",
      "Accuracy: 34.948% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(train_data_win, train_label_win, test_data_win, test_label_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
